{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3jbypmL2yNG",
        "outputId": "59a8a561-7163-41f6-814d-46119282a021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement localtunnel (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for localtunnel\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.13.0\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit\n",
        "!pip install -q localtunnel\n",
        "!pip install -q pyngrok\n",
        "!pip install faiss-cpu\n",
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.19.0\n",
        "!pip install keras==3.5.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5mHZ-tevwsd",
        "outputId": "1484186d-be2d-4f11-f4e9-94ebea578413"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.19.0 in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow==2.19.0) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow==2.19.0) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2025.11.12)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19.0) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19.0) (0.1.2)\n",
            "Collecting keras==3.5.0\n",
            "  Downloading keras-3.5.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras==3.5.0) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras==3.5.0) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras==3.5.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras==3.5.0) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras==3.5.0) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras==3.5.0) (0.18.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras==3.5.0) (0.5.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras==3.5.0) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras==3.5.0) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras==3.5.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras==3.5.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras==3.5.0) (0.1.2)\n",
            "Downloading keras-3.5.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.10.0\n",
            "    Uninstalling keras-3.10.0:\n",
            "      Successfully uninstalled keras-3.10.0\n",
            "Successfully installed keras-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDriP_tn3ipa",
        "outputId": "c9ef5833-e0ab-42f8-a616-854aff4ebfe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting localtunnel-py\n",
            "  Downloading localtunnel_py-0.1.1-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: aiohttp>=3.11.2 in /usr/local/lib/python3.12/dist-packages (from localtunnel-py) (3.13.2)\n",
            "Collecting asyncio>=3.4.3 (from localtunnel-py)\n",
            "  Downloading asyncio-4.0.0-py3-none-any.whl.metadata (994 bytes)\n",
            "Requirement already satisfied: nest-asyncio>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from localtunnel-py) (1.6.0)\n",
            "Requirement already satisfied: rich>=13.9.4 in /usr/local/lib/python3.12/dist-packages (from localtunnel-py) (13.9.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.11.2->localtunnel-py) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.11.2->localtunnel-py) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.11.2->localtunnel-py) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.11.2->localtunnel-py) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.11.2->localtunnel-py) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.11.2->localtunnel-py) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.11.2->localtunnel-py) (1.22.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.9.4->localtunnel-py) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.9.4->localtunnel-py) (2.19.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp>=3.11.2->localtunnel-py) (4.15.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->localtunnel-py) (0.1.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp>=3.11.2->localtunnel-py) (3.11)\n",
            "Downloading localtunnel_py-0.1.1-py3-none-any.whl (14 kB)\n",
            "Downloading asyncio-4.0.0-py3-none-any.whl (5.6 kB)\n",
            "Installing collected packages: asyncio, localtunnel-py\n",
            "Successfully installed asyncio-4.0.0 localtunnel-py-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install localtunnel-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIISvRJf4WGR",
        "outputId": "bab3f7e9-c2d8-41fb-a38d-c4aca916251d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit-lottie\n",
            "  Downloading streamlit_lottie-0.0.5-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: streamlit>=0.63 in /usr/local/lib/python3.12/dist-packages (from streamlit-lottie) (1.51.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=0.63->streamlit-lottie) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=0.63->streamlit-lottie) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=0.63->streamlit-lottie) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=0.63->streamlit-lottie) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit>=0.63->streamlit-lottie) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit>=0.63->streamlit-lottie) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=0.63->streamlit-lottie) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=0.63->streamlit-lottie) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit>=0.63->streamlit-lottie) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=0.63->streamlit-lottie) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit>=0.63->streamlit-lottie) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=0.63->streamlit-lottie) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit>=0.63->streamlit-lottie) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=0.63->streamlit-lottie) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit>=0.63->streamlit-lottie) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit>=0.63->streamlit-lottie) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit>=0.63->streamlit-lottie) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit>=0.63->streamlit-lottie) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=0.63->streamlit-lottie) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=0.63->streamlit-lottie) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=0.63->streamlit-lottie) (2.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.63->streamlit-lottie) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit>=0.63->streamlit-lottie) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit>=0.63->streamlit-lottie) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit>=0.63->streamlit-lottie) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-lottie) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-lottie) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-lottie) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit>=0.63->streamlit-lottie) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.63->streamlit-lottie) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=0.63->streamlit-lottie) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=0.63->streamlit-lottie) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=0.63->streamlit-lottie) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=0.63->streamlit-lottie) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=0.63->streamlit-lottie) (0.29.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit>=0.63->streamlit-lottie) (1.17.0)\n",
            "Downloading streamlit_lottie-0.0.5-py3-none-any.whl (802 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m802.4/802.4 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: streamlit-lottie\n",
            "Successfully installed streamlit-lottie-0.0.5\n",
            "Collecting streamlit-option-menu\n",
            "  Downloading streamlit_option_menu-0.4.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: streamlit>=1.36 in /usr/local/lib/python3.12/dist-packages (from streamlit-option-menu) (1.51.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.36->streamlit-option-menu) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.36->streamlit-option-menu) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.36->streamlit-option-menu) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.36->streamlit-option-menu) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.36->streamlit-option-menu) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.36->streamlit-option-menu) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.36->streamlit-option-menu) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.36->streamlit-option-menu) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.36->streamlit-option-menu) (5.29.5)\n",
            "Requirement already satisfied: pyarrow<22,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.36->streamlit-option-menu) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.36->streamlit-option-menu) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.36->streamlit-option-menu) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.36->streamlit-option-menu) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.36->streamlit-option-menu) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.36->streamlit-option-menu) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.36->streamlit-option-menu) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.36->streamlit-option-menu) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit>=1.36->streamlit-option-menu) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (2.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.36->streamlit-option-menu) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.36->streamlit-option-menu) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.36->streamlit-option-menu) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit>=1.36->streamlit-option-menu) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit>=1.36->streamlit-option-menu) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit>=1.36->streamlit-option-menu) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit>=1.36->streamlit-option-menu) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit>=1.36->streamlit-option-menu) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.36->streamlit-option-menu) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit>=1.36->streamlit-option-menu) (0.29.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit>=1.36->streamlit-option-menu) (1.17.0)\n",
            "Downloading streamlit_option_menu-0.4.0-py3-none-any.whl (829 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.3/829.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: streamlit-option-menu\n",
            "Successfully installed streamlit-option-menu-0.4.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (25.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement mpl_toolkits.mplot3d (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for mpl_toolkits.mplot3d\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install streamlit-lottie\n",
        "!pip install streamlit-option-menu\n",
        "!pip install transformers\n",
        "!pip install plotly\n",
        "!pip install matplotlib\n",
        "!pip install mpl_toolkits.mplot3d\n",
        "# !pip install tensorflow==2.15.0\n",
        "# !pip install keras==2.15.0  # Or the specific Keras version that corresponds to TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall streamlit_option_menu\n",
        "# !pip install streamlit_option_menu"
      ],
      "metadata": {
        "id": "uIAQNViO-Z5G"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VDiQaFfBfSF",
        "outputId": "2e460190-59ef-406f-ee20-0bc38497bb39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.2\n",
            "    Uninstalling transformers-4.57.2:\n",
            "      Successfully uninstalled transformers-4.57.2\n",
            "Successfully installed transformers-4.57.3\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceV8i9L-4cBO",
        "outputId": "63723806-8b5d-4f07-b9e7-e9a00678729e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import joblib\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from streamlit_lottie import st_lottie\n",
        "from PIL import Image\n",
        "from transformers import BertTokenizer, BertModel, MarianMTModel, MarianTokenizer, BartForConditionalGeneration, BartTokenizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from datetime import date\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "from streamlit_option_menu import option_menu\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import streamlit.components.v1 as components\n",
        "from transformers import pipeline\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "\n",
        "# Main app navigation\n",
        "with st.sidebar:\n",
        "    selected = option_menu(\n",
        "        \"Main Menu\",\n",
        "        [\"Home\", \"Insurance Risk & Claim\", \"Anomaly Detection\",\n",
        "         \"Customer Feedback\", \"Policy Translation\", \"Customer Segmentation\"],\n",
        "        icons=['house', 'shield', 'exclamation-triangle',\n",
        "               'chat', 'translate', 'people'],\n",
        "        menu_icon=\"cast\",\n",
        "        default_index=0\n",
        "    )\n",
        "\n",
        "# Home Page\n",
        "if selected == \"Home\":\n",
        "    st.title(\":red[AI-Powered Intelligent Insurance Risk Assessment and customer Insights System]\")\n",
        "\n",
        "    col1, col2, col3 = st.columns(3)\n",
        "\n",
        "    with col1:\n",
        "        if st.button(\"Insurance Risk & Claim\"):\n",
        "            selected = \"Insurance Risk & Claim\"\n",
        "        st.image(\"https://cdn-icons-png.flaticon.com/512/8438/8438971.png\", use_container_width=True)\n",
        "\n",
        "        if st.button(\"Anomaly Detection\"):\n",
        "            selected = \"Anomaly Detection\"\n",
        "        st.image(\"https://cdn-icons-png.flaticon.com/512/11331/11331293.png\", use_container_width=True)\n",
        "\n",
        "    with col2:\n",
        "        if st.button(\"Customer Feedback\"):\n",
        "            selected = \"Customer Feedback\"\n",
        "        st.image(\"https://static.vecteezy.com/system/resources/previews/041/317/536/original/3d-feedback-icon-on-transparent-background-png.png\", use_container_width=True)\n",
        "\n",
        "        if st.button(\"Policy Translation\"):\n",
        "            selected = \"Policy Translation\"\n",
        "        st.image(\"https://icon-library.com/images/translate-icon/translate-icon-4.jpg\", use_container_width=True)\n",
        "\n",
        "    with col3:\n",
        "        if st.button(\"Customer Segmentation\"):\n",
        "            selected = \"Customer Segmentation\"\n",
        "        st.image(\"https://cdn-icons-png.flaticon.com/512/2761/2761493.png\", use_container_width=True)\n",
        "\n",
        "# 1. Insurance Risk & Claim Page\n",
        "elif selected == \"Insurance Risk & Claim\":\n",
        "    # Read dataset for scaling\n",
        "    # Load models and encoders\n",
        "    logi_model = joblib.load('/content/drive/MyDrive/Captsone project/models/logi_model.pkl')\n",
        "    logis_model = joblib.load('/content/drive/MyDrive/Captsone project/models/logistic_regression_model.pkl')\n",
        "    loaded_model = load_model('/content/drive/MyDrive/Captsone project/models/my_model.keras')\n",
        "    with open('/content/drive/MyDrive/Captsone project/models/onehot_encoder.pkl', 'rb') as file:\n",
        "        onehot_encoder = pickle.load(file)\n",
        "\n",
        "    # Read dataset for scaling\n",
        "    df1 = pd.read_csv('/content/drive/MyDrive/Captsone project/Data/df1-synthetic_insurance_dataset_fixed.csv')\n",
        "\n",
        "    # Get min and max values for scaling\n",
        "    min_values = df1[['Annual_Income', 'Premium_Amount', 'Claim_Amount']].min()\n",
        "    max_values = df1[['Annual_Income', 'Premium_Amount', 'Claim_Amount']].max()\n",
        "\n",
        "    # Fit the scaler\n",
        "    min_max_values = np.array([min_values, max_values])\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaler.fit(min_max_values)\n",
        "\n",
        "    st.title(\"Insurance Risk & Claim Analysis\")\n",
        "    tab1, tab2, tab3 = st.tabs([\"Risk Score Prediction\", \"Claim Prediction\", \"Fraud Claim Prediction\"])\n",
        "\n",
        "    with tab1:\n",
        "        st.header(\"Risk Score Prediction\")\n",
        "        # # Load models\n",
        "        # Risk_model = joblib.load(r'/content/drive/MyDrive/Captsone project/Insurance_Risk_Claim_Dataset/Risk_model.pkl')\n",
        "        # scaler =  joblib.load(r'/content/drive/MyDrive/Captsone project/Insurance_Risk_Claim_Dataset/scaler_Risk.pkl')\n",
        "        # with open('/content/drive/MyDrive/Captsone project/Insurance_Risk_Claim_Dataset/onehot_Risk_encoder.pkl', 'rb') as file:\n",
        "        #     onehot_encoder = pickle.load(file)\n",
        "\n",
        "        # User inputs\n",
        "        col1, col2,col3, col4 = st.columns(4)\n",
        "        with col1:\n",
        "            customer_age = st.number_input('Customer Age', min_value=18, max_value=80, step=1, key='age_risk')\n",
        "            annual_income_raw = st.number_input('Annual Income', key='income_risk')\n",
        "            vehicle_age_property_age = st.number_input('Vehicle/Property Age', min_value=0, max_value=30, step=1, key='vehicle_age_risk')\n",
        "\n",
        "\n",
        "        with col2:\n",
        "            claim_history = st.number_input('Claim History', min_value=0, max_value=5, step=1, key='claim_history_risk')\n",
        "            premium_amount_raw = st.number_input('Premium Amount', key='premium_risk')\n",
        "\n",
        "\n",
        "        with col3:\n",
        "            claim_amount_raw = st.number_input('Claim Amount', key='claim_amt_risk')\n",
        "            fraudulent_claim = st.number_input('Fraudulent Claim', key='fraud_risk')\n",
        "\n",
        "        with col4:\n",
        "            policy_type = st.selectbox('Policy Type', ['Auto', 'Health', 'Life', 'Property'], key='policy_type_risk')\n",
        "            gender = st.selectbox('Gender', ['Female', 'Male', 'Other'], key='gender_risk')\n",
        "\n",
        "        categorical_data = pd.DataFrame({'Policy_Type': [policy_type], 'Gender': [gender]})\n",
        "        encoded_data = onehot_encoder.transform(categorical_data).reshape(1, -1)\n",
        "\n",
        "        scaled_features = scaler.transform([[annual_income_raw, premium_amount_raw, claim_amount_raw]])\n",
        "        annual_income, premium_amount, claim_amount = scaled_features[0]\n",
        "\n",
        "        new_data = np.array([[customer_age, annual_income, vehicle_age_property_age, claim_history, premium_amount, claim_amount, fraudulent_claim]])\n",
        "        final_input = np.concatenate([new_data, encoded_data], axis=1)\n",
        "\n",
        "        if st.button('Predict Advanced Fraud Risk'):\n",
        "            try:\n",
        "                predictions = loaded_model.predict(final_input)\n",
        "                predicted_class = np.argmax(predictions, axis=1)\n",
        "                risk_labels = {0: 'Low (0)', 1: 'Medium (1)', 2: 'High (2)'}\n",
        "                risk_score = risk_labels.get(predicted_class[0], 'Unknown')\n",
        "                st.write(f\"Predicted class: {predicted_class[0]}\")\n",
        "                st.write(f\"Risk Score: {risk_score}\")\n",
        "                st.write(f\"Class probabilities: {predictions}\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "    with tab2:\n",
        "        st.header(\"Claim Prediction\")\n",
        "        # Claim_Amount_model = joblib.load(r'/content/drive/MyDrive/Captsone project/Insurance_Risk_Claim_Dataset/Claim_Amount_model.pkl')\n",
        "        # with open('/content/drive/MyDrive/Captsone project/Insurance_Risk_Claim_Dataset/onehot_Claim_encoder.pkl', 'rb') as file:\n",
        "        #     onehot_encoder = pickle.load(file)\n",
        "        # with open('/content/drive/MyDrive/Captsone project/Insurance_Risk_Claim_Dataset/lbl_Claim_encoder.pkl', 'rb') as file:\n",
        "        #     label_encoder = pickle.load(file)\n",
        "\n",
        "        # User inputs\n",
        "        col1, col2,col3 = st.columns(3)\n",
        "        with col1:\n",
        "            customer_age = st.number_input('Customer Age', min_value=18, max_value=80, step=1, key='age_claim')\n",
        "            annual_income_raw = st.number_input('Annual Income', key='income_claim')\n",
        "            Vehicle_Age_Property_Age = st.number_input('Vehicle/Property Age', min_value=0, max_value=30, step=1, key='vehicle_age_claim')\n",
        "\n",
        "\n",
        "        with col2:\n",
        "            claim_history = st.number_input('Claim History', min_value=0, max_value=5, step=1, key='claim_history_claim')\n",
        "            premium_amount_raw = st.number_input('Premium Amount', key='premium_claim')\n",
        "\n",
        "        with col3:\n",
        "            policy_type = st.selectbox('Policy Type', ['Auto', 'Health', 'Life', 'Property'], key='policy_type_claim')\n",
        "            gender = st.selectbox('Gender', ['Female', 'Male', 'Other'], key='gender_claim')\n",
        "\n",
        "\n",
        "        policy_type_encoded = [1 if policy_type == p else 0 for p in ['Auto', 'Health', 'Life', 'Property']]\n",
        "        gender_encoded = [1 if gender == g else 0 for g in ['Female', 'Male', 'Other']]\n",
        "\n",
        "        scaled_features = scaler.transform([[annual_income_raw, premium_amount_raw, 0]])  # Add a placeholder for Claim_Amount\n",
        "        annual_income, premium_amount, _ = scaled_features[0]  # Ignore the third value\n",
        "\n",
        "        new_data = pd.DataFrame({\n",
        "            'Customer_Age': [customer_age],\n",
        "            'Policy_Type_Auto': [policy_type_encoded[0]],\n",
        "            'Policy_Type_Health': [policy_type_encoded[1]],\n",
        "            'Policy_Type_Life': [policy_type_encoded[2]],\n",
        "            'Policy_Type_Property': [policy_type_encoded[3]],\n",
        "            'Gender_Female': [gender_encoded[0]],\n",
        "            'Gender_Male': [gender_encoded[1]],\n",
        "            'Gender_Other': [gender_encoded[2]],\n",
        "            'Annual_Income': [annual_income],\n",
        "            'Vehicle_Age_Property_Age': [vehicle_age_property_age],\n",
        "            'Premium_Amount': [premium_amount],\n",
        "            'Claim_History': [claim_history]\n",
        "        })\n",
        "\n",
        "        if st.button('Predicts Claim Prediction'):\n",
        "            try:\n",
        "                predictions = logis_model.predict_proba(new_data)\n",
        "                predicted_class = logis_model.predict(new_data)\n",
        "                Claim_labels = {0: 'Filed Claim 2 or Below / did not file a claim Single time (0)', 1: 'Already Filed Claims More than 2 (1)'}\n",
        "                Claim_score = Claim_labels.get(predicted_class[0], 'Unknown')\n",
        "                st.write(f\"Predicted class: {predicted_class[0]}\")\n",
        "                st.write(f\"Claim Score: {Claim_score}\")\n",
        "                st.write(f\"Class probabilities: {predictions}\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "    with tab3:\n",
        "        st.header('Fraud Claim Prediction')\n",
        "        # Fraud_Amount_model = joblib.load(r'/content/drive/MyDrive/Captsone project/Insurance_Risk_Claim_Dataset/Fraud_claim_model.pkl')\n",
        "        # # scaler_Fraud =  joblib.load('E:\\Captsone project\\Insurance_Risk_Claim_Dataset\\scaler_Risk.pkl')\n",
        "        # with open('/content/drive/MyDrive/Captsone project/Insurance_Risk_Claim_Dataset/scaler_Risk.pkl', 'rb') as file:\n",
        "        #     scaler_Fraud = pickle.load(file)\n",
        "        # with open('/content/drive/MyDrive/Captsone project/Insurance_Risk_Claim_Dataset/lbl_Claim_encoder.pkl', 'rb') as file:\n",
        "        #     label_encoder = pickle.load(file)\n",
        "\n",
        "        # User inputs\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            vehicle_age_property_age = st.number_input('Vehicle_Age_Property_Age', min_value=0, max_value=30, step=1, key='vehicle_age_Fraud')\n",
        "            claim_amount_raw = st.number_input('Claim_Amount', key='claim_amt_Fraud')\n",
        "\n",
        "        with col2:\n",
        "            premium_amount_raw = st.number_input('Premium_Amount', key='premium_Fraud')\n",
        "            claim_history = st.number_input('Claim_History', min_value=0, max_value=5, step=1, key='claim_history_Fraud')\n",
        "\n",
        "\n",
        "        scaled_features = scaler.transform([[0, premium_amount_raw, claim_amount_raw]])  # Placeholder for missing feature\n",
        "        _, premium_amount, claim_amount = scaled_features[0]  # Ignore the first value\n",
        "\n",
        "\n",
        "        new_data = pd.DataFrame({\n",
        "            'Vehicle_Age_Property_Age': [vehicle_age_property_age],\n",
        "            'Premium_Amount': [premium_amount],\n",
        "            'Claim_Amount': [claim_amount],\n",
        "            'Claim_History': [claim_history]\n",
        "        })\n",
        "\n",
        "        if st.button('Predict Simple Fraud'):\n",
        "            try:\n",
        "                predictions = logi_model.predict_proba(new_data)\n",
        "                predicted_class = logi_model.predict(new_data)\n",
        "                risk_labels = {0: 'Genuine (0)', 1: 'Fraud (1)'}\n",
        "                risk_score = risk_labels.get(predicted_class[0], 'Unknown')\n",
        "                st.write(f\"Predicted class: {predicted_class[0]}\")\n",
        "                st.write(f\"Risk Score: {risk_score}\")\n",
        "                st.write(f\"Class probabilities: {predictions}\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"An error occurred: {e}\")\n",
        "\n",
        "# 2. Anomaly Detection Page\n",
        "elif selected == \"Anomaly Detection\":\n",
        "    st.title(\"Anomaly Detection in Insurance Claims\")\n",
        "\n",
        "    tab1, tab2, tab3 = st.tabs([\"Claim Period Analysis\", \"Anomaly Score with graph\", \"Fraud Detection\"])\n",
        "\n",
        "    with tab1:\n",
        "        st.header(\"Claim Period Analysis\")\n",
        "        df = pd.read_csv(r'/content/drive/MyDrive/Captsone project/Data/3_fraudulent_insurance_claims.csv')\n",
        "\n",
        "        early_threshold = st.slider(\"Early Claim Threshold (days)\", 30, 180, 90)\n",
        "        expired_threshold = st.slider(\"Expired Claim Threshold (days)\", 365, 1750, 365)\n",
        "\n",
        "        df[\"Early_Claim_Flag\"] = df[\"Days_Since_Issue\"] < early_threshold\n",
        "        df[\"Expired_Claim_Flag\"] = df[\"Days_Since_Issue\"] > expired_threshold\n",
        "\n",
        "        early_claims = df[df[\"Early_Claim_Flag\"]]\n",
        "        expired_claims = df[df[\"Expired_Claim_Flag\"]]\n",
        "\n",
        "        st.write(f\"Early Claims: {len(early_claims)}\")\n",
        "        st.write(f\"Expired Claims: {len(expired_claims)}\")\n",
        "\n",
        "        fig = px.pie(names=[\"Early\", \"Expired\", \"Normal\"],\n",
        "                    values=[len(early_claims), len(expired_claims), len(df) - len(early_claims) - len(expired_claims)])\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "    with tab2:\n",
        "        st.header(\"Anomaly Score with graph\")\n",
        "        data = pd.read_csv(r'/content/drive/MyDrive/Captsone project/Data/3_feature_of_fraud_claim.csv').drop(columns=['Unnamed: 0'])\n",
        "        concated_df = pd.read_csv(r'/content/drive/MyDrive/Captsone project/Data/3_fraudulent_insurance_claims.csv')\n",
        "\n",
        "        anomaly_percent = st.slider(\"Anomaly Percentile\", 1, 50, 33)\n",
        "\n",
        "        iso_forest = IsolationForest(n_estimators=200, random_state=42)\n",
        "        iso_forest.fit(data)\n",
        "\n",
        "        concated_df[\"Anomaly_Score\"] = iso_forest.decision_function(data)\n",
        "        threshold = np.percentile(concated_df[\"Anomaly_Score\"], anomaly_percent)\n",
        "        concated_df[\"Anomaly_Label\"] = (concated_df[\"Anomaly_Score\"] < threshold).astype(int)\n",
        "\n",
        "        anomalies = concated_df[concated_df[\"Anomaly_Label\"] == 1]\n",
        "        st.write(f\"Detected {len(anomalies)} anomalies\")\n",
        "\n",
        "        fig = px.histogram(concated_df, x=\"Anomaly_Score\")\n",
        "        fig.add_vline(x=threshold, line_dash=\"dash\", line_color=\"red\")\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "    with tab3:\n",
        "        st.header(\"Fraud Detection\")\n",
        "        # Load the trained model\n",
        "        log_reg = joblib.load('/content/drive/MyDrive/Captsone project/models/logistic_regression_fraud_model.pkl')\n",
        "\n",
        "        # Read the dataset\n",
        "        df1 = pd.read_csv('/content/drive/MyDrive/Captsone project/Data/df3_upd_labels.csv')\n",
        "\n",
        "        # Get min and max values for the required columns\n",
        "        min_values = df1[['Annual_Income', 'Claim_Amount']].min()\n",
        "        max_values = df1[['Annual_Income', 'Claim_Amount']].max()\n",
        "        min_max_values = np.array([min_values, max_values])\n",
        "\n",
        "        # Fit the scaler on known ranges\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        scaler.fit(min_max_values)\n",
        "\n",
        "        # Feature list used during training\n",
        "        training_features = ['Claim_Amount', 'Suspicious_Flags', 'Claim_Type_Home_Damage', 'Claim_Type_Medical',\n",
        "                             'Claim_Type_Vehicle', 'Claim_Year', 'Claim_Month', 'Claim_Day', 'Annual_Income',\n",
        "                             'Claim_to_Income_Ratio', 'Days_Since_Issuance', 'Short_Period_Claim', 'Isolation_Anomaly',\n",
        "                             'policy_issue_Year', 'policy_issue_Month', 'policy_issue_Day']\n",
        "\n",
        "        # Function to calculate days since policy issuance\n",
        "        def calculate_days_since_issuance(claim_date, policy_issue_date):\n",
        "            delta = claim_date - policy_issue_date\n",
        "            return delta.days\n",
        "\n",
        "        # Function to prepare input data\n",
        "        def prepare_input_data(user_input):\n",
        "            # Scale Claim_Amount and Annual_Income\n",
        "            scaled_values = scaler.transform([[user_input['Annual_Income'], user_input['Claim_Amount']]])\n",
        "            user_input['Annual_Income'], user_input['Claim_Amount'] = scaled_values[0]\n",
        "\n",
        "            # Map Claim_Type to one-hot encoded columns\n",
        "            claim_type_mapping = {\n",
        "                'Home Damage': [1, 0, 0],\n",
        "                'Medical': [0, 1, 0],\n",
        "                'Vehicle': [0, 0, 1]\n",
        "            }\n",
        "            user_input['Claim_Type_Home_Damage'], user_input['Claim_Type_Medical'], user_input['Claim_Type_Vehicle'] = claim_type_mapping[user_input['Claim_Type']]\n",
        "\n",
        "            # Calculate engineered features\n",
        "            user_input['Claim_to_Income_Ratio'] = user_input['Claim_Amount'] / user_input['Annual_Income']\n",
        "            user_input['Suspicious_Flags'] = 1 if user_input['Claim_to_Income_Ratio'] > 0.5 else 0\n",
        "\n",
        "            # Calculate days since policy issuance\n",
        "            claim_date = date(user_input['Claim_Year'], user_input['Claim_Month'], user_input['Claim_Day'])\n",
        "            policy_issue_date = date(user_input['policy_issue_Year'], user_input['policy_issue_Month'], user_input['policy_issue_Day'])\n",
        "            user_input['Days_Since_Issuance'] = calculate_days_since_issuance(claim_date, policy_issue_date)\n",
        "\n",
        "            # Determine if it's a short-period claim\n",
        "            user_input['Short_Period_Claim'] = 1 if user_input['Days_Since_Issuance'] < 365 else 0\n",
        "\n",
        "            # Updated Isolation Anomaly detection using refit approach\n",
        "            sample_data = pd.DataFrame([[user_input['Claim_Amount'], user_input['Claim_Year'], user_input['Claim_Month'], user_input['Claim_Day'], user_input['Claim_to_Income_Ratio'], user_input['Days_Since_Issuance']]])\n",
        "            iso_forest = IsolationForest(contamination=0.20, random_state=42)\n",
        "            user_input['Isolation_Anomaly'] = iso_forest.fit_predict(sample_data)[0]\n",
        "\n",
        "            # Align feature names and order\n",
        "            input_data = pd.DataFrame([user_input])[training_features]\n",
        "\n",
        "            return input_data, user_input, claim_date, policy_issue_date\n",
        "\n",
        "        # Collect user input\n",
        "        claim_amount = st.number_input('Claim Amount', min_value=0.00)\n",
        "        claim_type = st.selectbox('Claim Type', ['Home Damage', 'Medical', 'Vehicle'])\n",
        "        claim_year = st.number_input('Claim Year', min_value=2000, max_value=2100)\n",
        "        claim_month = st.number_input('Claim Month', min_value=1, max_value=12)\n",
        "        claim_day = st.number_input('Claim Day', min_value=1, max_value=31)\n",
        "        annual_income = st.number_input('Annual Income', min_value=0.00)\n",
        "        policy_issue_year = st.number_input('Policy Issue Year', min_value=2000, max_value=2100)\n",
        "        policy_issue_month = st.number_input('Policy Issue Month', min_value=1, max_value=12)\n",
        "        policy_issue_day = st.number_input('Policy Issue Day', min_value=1, max_value=31)\n",
        "\n",
        "        # Prepare input and predict if the user clicks the button\n",
        "        if st.button('Predict Fraud Risk'):\n",
        "            user_input = {\n",
        "                'Claim_Amount': claim_amount,\n",
        "                'Claim_Type': claim_type,\n",
        "                'Claim_Year': claim_year,\n",
        "                'Claim_Month': claim_month,\n",
        "                'Claim_Day': claim_day,\n",
        "                'Annual_Income': annual_income,\n",
        "                'policy_issue_Year': policy_issue_year,\n",
        "                'policy_issue_Month': policy_issue_month,\n",
        "                'policy_issue_Day': policy_issue_day\n",
        "            }\n",
        "\n",
        "            prepared_data, user_input_features, claim_date, policy_issue_date = prepare_input_data(user_input)\n",
        "            prediction = log_reg.predict(prepared_data)\n",
        "            result = 'FRAUD' if prediction[0] == 1 else 'GENUINE'\n",
        "            st.success(f'Predicted class: {result}')\n",
        "\n",
        "            st.write(f\"Claim Date: {claim_date}\")\n",
        "            st.write(f\"Policy Issue Date: {policy_issue_date}\")\n",
        "            st.write(f\"Days Since Issuance: {user_input_features['Days_Since_Issuance']}\")\n",
        "\n",
        "            # Show calculated feature values\n",
        "            st.write(f\"Claim to Income Ratio: {user_input_features['Claim_to_Income_Ratio']}\")\n",
        "            st.write(f\"Suspicious Flags: {'True' if user_input_features['Suspicious_Flags'] == 1 else 'False'}\")\n",
        "            st.write(f\"Short Period Claim: {'True' if user_input_features['Short_Period_Claim'] == 1 else 'False'}\")\n",
        "            st.write(f\"Isolation Anomaly: {'Anomaly' if user_input_features['Isolation_Anomaly'] == -1 else 'Normal'}\")\n",
        "            st.write(f\"Scaled Annual Income: {user_input_features['Annual_Income']}\")\n",
        "            st.write(f\"Scaled Claim Amount: {user_input_features['Claim_Amount']}\")\n",
        "\n",
        "\n",
        "\n",
        "# 3. Customer Feedback Analysis\n",
        "elif selected == \"Customer Feedback\":\n",
        "    st.title(\"Customer Feedback Analysis\")\n",
        "\n",
        "    # Load models\n",
        "    tokenizer = joblib.load(r'/content/drive/MyDrive/Captsone project/models/_Models_tokenizer_bert_textreview.pkl')\n",
        "    model = joblib.load(r'/content/drive/MyDrive/Captsone project/models/_Models_torch_bert_textreview.pkl')\n",
        "    rfc_model = joblib.load(r'/content/drive/MyDrive/Captsone project/models/_Models_Prefect_RandomForestclassifer_Model_for_ReviewText.pkl')\n",
        "\n",
        "    # Sample reviews\n",
        "    reviews = {\n",
        "        \"Positive\": \"Excellent service, very helpful staff, quick response times\",\n",
        "        \"Neutral\": \"Itself turn law purpose budget require course\",\n",
        "        \"Negative\": \"Terrible service, slow response, unhelpful staff\"\n",
        "    }\n",
        "\n",
        "    review_text = st.selectbox(\"Select a sample review or enter your own:\",\n",
        "                             list(reviews.values()) + [\"Custom\"])\n",
        "    if review_text == \"Custom\":\n",
        "        review_text = st.text_area(\"Enter your review text:\")\n",
        "\n",
        "    if st.button(\"Analyze Sentiment\"):\n",
        "        inputs = tokenizer(review_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            output_attentions=True\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "        prediction = rfc_model.predict([embedding])[0]\n",
        "        st.write(\"rfc_model:\", prediction)\n",
        "        if prediction == \"Positive\":\n",
        "            st.success(\"Positive Sentiment\")\n",
        "        elif prediction == \"Neutral\":\n",
        "            st.warning(\"Neutral Sentiment\")\n",
        "        else:\n",
        "            st.error(\"Negative Sentiment\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 4. Policy Translation Page\n",
        "elif selected == \"Policy Translation\":\n",
        "    st.title(\"Policy Translation & Summarization\")\n",
        "\n",
        "    # Load models\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    #Frenchh to English Models\n",
        "    fr_en_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "    fr_en_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-fr-en\", device_map='auto')\n",
        "    #English to French Models\n",
        "    en_fr_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
        "    en_fr_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\", device_map='auto')\n",
        "    #Tamil to English Models\n",
        "    ta_en_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-mul-en\")\n",
        "    ta_en_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-mul-en\", device_map='auto')\n",
        "    #English to Hindi Models\n",
        "    en_hi_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
        "    en_hi_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\", device_map='auto')\n",
        "    #Hindi to English Models\n",
        "    hi_en_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-hi-en\")\n",
        "    hi_en_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-hi-en\", device_map='auto')\n",
        "    #Spanish to English Model\n",
        "    es_en_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-es-en\")\n",
        "    es_en_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-es-en\").to(device)\n",
        "    #English to Spanish Model\n",
        "    en_es_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\")\n",
        "    en_es_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-es\").to(device)\n",
        "    #summarize Models\n",
        "    summarizer_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "    summarizer_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\", device_map='auto')\n",
        "\n",
        "    text = st.text_area(\"Enter text to translate or summarize:\")\n",
        "    operation = st.radio(\"Select operation:\",\n",
        "                        [\"Translate French to English\",\n",
        "                         \"Translate English to French\",\n",
        "                         \"Translate Spanish to English\",\n",
        "                         \"Translate English to Spanish\",\n",
        "                         \"Translate Tamil to English\",\n",
        "                         \"Translate English to Hindi\",\n",
        "                         \"Translate Hindi to English\",\n",
        "                         \"Summarize Text\"])\n",
        "\n",
        "    if st.button(\"Process\"):\n",
        "      if operation == \"Translate French to English\":\n",
        "        inputs = fr_en_tokenizer.encode(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "        outputs = fr_en_model.generate(inputs)\n",
        "        translated = fr_en_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        st.success(f\"Translated: {translated}\")\n",
        "      elif operation == \"Translate English to French\":\n",
        "        inputs = en_fr_tokenizer.encode(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "        outputs = en_fr_model.generate(inputs)\n",
        "        translated = en_fr_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        st.success(f\"Translated: {translated}\")\n",
        "      elif operation == \"Translate Spanish to English\":\n",
        "        inputs = es_en_tokenizer.encode(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "        outputs = es_en_model.generate(inputs)\n",
        "        translated = es_en_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        st.success(f\"Translated: {translated}\")\n",
        "      elif operation == \"Translate English to Spanish\":\n",
        "        inputs = en_es_tokenizer.encode(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "        outputs = en_es_model.generate(inputs)\n",
        "        translated = en_es_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        st.success(f\"Translated: {translated}\")\n",
        "      elif operation == \"Translate Tamil to English\":\n",
        "        inputs = ta_en_tokenizer.encode(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "        outputs = ta_en_model.generate(inputs)\n",
        "        translated = ta_en_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        st.success(f\"Translated: {translated}\")\n",
        "      elif operation == \"Translate English to Hindi\":\n",
        "        inputs = en_hi_tokenizer.encode(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "        outputs = en_hi_model.generate(inputs)\n",
        "        translated = en_hi_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        st.success(f\"Translated: {translated}\")\n",
        "    elif operation == \"Translate Hindi to English\":\n",
        "        inputs = hi_en_tokenizer.encode(text, return_tensors=\"pt\", truncation=True).to(device)\n",
        "        outputs = hi_en_model.generate(inputs)\n",
        "        translated = hi_en_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        st.success(f\"Translated: {translated}\")\n",
        "    else:\n",
        "        inputs = summarizer_tokenizer.encode(text, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "        summary_ids = summarizer_model.generate(inputs, max_length=150, min_length=50).to(device)\n",
        "        summary = summarizer_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        st.success(f\"Summary: {summary}\")\n",
        "\n",
        "# 5. Customer Segmentation Page\n",
        "elif selected == \"Customer Segmentation\":\n",
        "    st.title(\"Customer Segmentation\")\n",
        "\n",
        "    # Load models\n",
        "    scaler = joblib.load(r\"/content/drive/MyDrive/Captsone project/models/5_scaler_unsupervised.pkl\")\n",
        "    pca = joblib.load(r\"/content/drive/MyDrive/Captsone project/models/5_PCA_unsupervised.pkl\")\n",
        "    kmeans = joblib.load(r\"/content/drive/MyDrive/Captsone project/models/5_Kmeans_Unsupervised.pkl\")\n",
        "\n",
        "    tab1, tab2 = st.tabs([\"Cluster Visualization\", \"Predict Cluster\"])\n",
        "\n",
        "    with tab1:\n",
        "        df = pd.read_csv(r\"/content/drive/MyDrive/Captsone project/Data/5_Unsupervised_customer_data.csv\")\n",
        "        df_pca = pca.transform(df.iloc[:, 1:])\n",
        "        df[\"Cluster\"] = kmeans.labels_\n",
        "\n",
        "        fig = px.pie(df, names=\"Cluster\", title=\"Cluster Distribution\")\n",
        "        st.plotly_chart(fig)\n",
        "\n",
        "        fig = plt.figure(figsize=(10, 6))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        ax.scatter(df_pca[:, 0], df_pca[:, 1], df_pca[:, 2], c=df[\"Cluster\"])\n",
        "        st.pyplot(fig)\n",
        "\n",
        "    with tab2:\n",
        "        age = st.number_input(\"Age\", min_value=18, max_value=100)\n",
        "        income = st.number_input(\"Income\", min_value=10000)\n",
        "        Number_of_Active_Policies = st.number_input(\"Number of Active Policies\", min_value=1)\n",
        "        total_premium_paid = st.number_input(\"Total Premium Paid\", min_value=500)\n",
        "        claim_frequency = st.number_input(\"Claim Frequency\", min_value=0)\n",
        "        policy_upgrades = st.number_input(\"Policy Upgrades\", min_value=0)\n",
        "\n",
        "        if st.button(\"Predict Cluster\"):\n",
        "            new_data = pd.DataFrame([[age, income, Number_of_Active_Policies, total_premium_paid, claim_frequency, policy_upgrades]],\n",
        "                                  columns=[\"Age\", \"Income\", \"Number_of_Active_Policies\", \"Total_Premium_Paid\", \"Claim_Frequency\", \"Policy_Upgrades\"])\n",
        "\n",
        "            new_data[[\"Income\", \"Total_Premium_Paid\"]] = scaler.transform(new_data[[\"Income\", \"Total_Premium_Paid\"]])\n",
        "            new_data_pca = pca.transform(new_data)\n",
        "            cluster = kmeans.predict(new_data_pca)[0]\n",
        "\n",
        "            st.success(f\"Predicted Cluster: {cluster}\")\n",
        "\n",
        "            cluster_info = {\n",
        "                0: \"High-value loyal customers\",\n",
        "                1: \"Young and growing customers\",\n",
        "                2: \"Risky high-claim customers\",\n",
        "                3: \"Low engagement customers\"\n",
        "            }\n",
        "\n",
        "            st.write(f\"Cluster Characteristics: {cluster_info.get(cluster, 'Unknown')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "I1ZL7V3q7yo9"
      },
      "outputs": [],
      "source": [
        "# !streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LuPl2hP68Omv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab79d3fc-7a91-4b58-abce-e834875e5679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app is running at NgrokTunnel: \"https://2ba5618db33a.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Replace 'YOUR_AUTHTOKEN' with your actual ngrok authtoken\n",
        "ngrok.set_auth_token(\"3289CH2MUbiPl44i8OBDySuT999_5GJcmNEw54vxoj8joieDv\")\n",
        "\n",
        "# Run the Streamlit app in the background\n",
        "!streamlit run app.py &>/dev/null&\n",
        "\n",
        "# Create a public URL using ngrok\n",
        "try:\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(f\"Streamlit app is running at {public_url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Trying to run with localtunnel\")\n",
        "    !streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ],
      "metadata": {
        "id": "Y6aWaIjHv9-9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}